{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating regression lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Understand what is meant by the errors of a regression line\n",
    "* Understand how to calculate the error at a given point\n",
    "* Understand how to calculate RSS and why we use it as a metric to evaluate a regression line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, so far we have seen that our regression lines make an estimation of our values of $y$.  For example, we saw how to use a regression line to estimate movie's expected revenue given a budget.  Our regression lines are described by two different variables, $m$, which represents the slope of the line and $b$ which is the value of $y$ when $x$ is zero.\n",
    "\n",
    "So far we have been rather fast and loose with choosing our regression line.  We have either used data where our regression line perfectly matched our data, or have simply used the eyeball test to see that our regression line made sense.  Well today, we going further.  In this lesson, we'll learn how to evaluate the accuracy of a regression line, and eventually how to to choose a better regression line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure the accuracy of a regression line, we see how closely our regression line matches the data we have.  Let's find out what this means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_show = {'x': 0, 'y': 100}\n",
    "second_show = {'x': 100, 'y': 150}\n",
    "third_show = {'x': 200, 'y': 600}\n",
    "fourth_show = {'x': 400, 'y': 700}\n",
    "\n",
    "shows = [first_show, second_show, third_show, fourth_show]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, let's set a roughshod regression line simply by drawing a line between our first and last points.  Eventually, we'll improve this regression line, so it's ok that we don't use a rigorous technique right now.  We can use our `build_regression_line` function to do so.  You can view the code directly here.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b': 100.0, 'm': 1.5}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from linear_equations import build_regression_line\n",
    "x_values = list(map(lambda show: show['x'],shows))\n",
    "y_values = list(map(lambda show: show['y'],shows))\n",
    "regression_line = build_regression_line(x_values, y_values)\n",
    "regression_line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot our regression line as the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "line",
         "name": "line function",
         "x": [
          0,
          100,
          200,
          400
         ],
         "y": [
          100,
          250,
          400,
          700
         ]
        },
        {
         "mode": "markers",
         "name": "data",
         "text": [],
         "x": [
          0,
          100,
          200,
          400
         ],
         "y": [
          100,
          150,
          600,
          700
         ]
        }
       ],
       "layout": {}
      },
      "text/html": [
       "<div id=\"f4823a6f-8fe6-4a3a-9476-14fd5073178c\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"f4823a6f-8fe6-4a3a-9476-14fd5073178c\", [{\"x\": [0, 100, 200, 400], \"y\": [100.0, 250.0, 400.0, 700.0], \"mode\": \"line\", \"name\": \"line function\"}, {\"x\": [0, 100, 200, 400], \"y\": [100, 150, 600, 700], \"mode\": \"markers\", \"name\": \"data\", \"text\": []}], {}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"f4823a6f-8fe6-4a3a-9476-14fd5073178c\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"f4823a6f-8fe6-4a3a-9476-14fd5073178c\", [{\"x\": [0, 100, 200, 400], \"y\": [100.0, 250.0, 400.0, 700.0], \"mode\": \"line\", \"name\": \"line function\"}, {\"x\": [0, 100, 200, 400], \"y\": [100, 150, 600, 700], \"mode\": \"markers\", \"name\": \"data\", \"text\": []}], {}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from graph import m_b_trace, plot, trace_values\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "init_notebook_mode(connected=True)\n",
    "data_trace = trace_values(x_values, y_values)\n",
    "regression_trace = m_b_trace(regression_line['m'], regression_line['b'], x_values)\n",
    "plot([regression_trace, data_trace])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that is what our regression line looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_regression_formula(x):\n",
    "    return 1.5(x) + 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so now that we see what our regression line looks like, let's highlight how well our regression line matches our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./regression-scatter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure we understand what the above chart is displaying.  Take a look at that first red line.  It's showing that our regression formula does not perfectly predict our data.  More concretely, at that spot, $x = 100$, our regression line is predicting that the value of $y$ will be 250.  However the point below our regression line shows the actual value of y when x = 100 which is 150.  So our regression line is off. \n",
    "\n",
    "Each point where our regression line's prediction differs from the actual data is called an error.  So that is what the reds lines are indictating -- the distance between our regression line's predicted value and the actual value.  Or in other words, the red lines are visually displaying the size of each error.\n",
    "\n",
    "Now let's measure the size of that error.  We can say that our error, at point $x = 100$, is the actual $y$ minus expected $y$, which translates to $150 - 250 = -100$.  Note that even without the graph we can see this by using our formula for the regression line $y = 1.5x + 100$.  Setting $x$ equal to 100, we see that $y = 1.5 * 100 + 100 = 250$.  And we have the actual data point of (100, 150).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refining our Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have explained how to calculate an error given a regression line and data, let's see how we can express this concept with mathematical notation.  \n",
    "\n",
    "To start, we want to use notation to distinguish between two things: our predicted $y$ values and our actual $y$ values.  So far we have defined our regression function as $y = mx + b$.  Where for a given value of $x$, we can calculate the value of $y$.  However, this is not totally accurate - as our regression line is not calculating the actual value of $y$ but the *expected * value of $y$. So let's indicate this, by changing our regression line formula to look like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\overline{y} = \\overline{m}x + \\overline{b}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those little dashes over the $y$, $m$ and $b$ are called hats.  So our function reads as y-hat equals m-hat times x plus b-hat.  These hats indicate that this formula does not give us the actual value of $y$, but simply our estimated value of $y$.  And that this estimated value of $y$ is based on our estimated values of $m$ and $b$. \n",
    "> Note that $x$ is not a predicted value.  Why is this?  Well, we are *providing* a value $x$, for example a movie budget, not predicting it.  So we are *providing* a value of $x$ and asking it to *predict* a value of $y$.  \n",
    "\n",
    "Now remember that we were given some real data as well.  This means that we do have actual points for $x$ and $y$, which looks like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_show = {'x': 0, 'y': 100}\n",
    "second_show = {'x': 100, 'y': 150}\n",
    "third_show = {'x': 200, 'y': 600}\n",
    "fourth_show = {'x': 400, 'y': 700}\n",
    "\n",
    "shows = [first_show, second_show, third_show, fourth_show]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we represent our actual values of $y$? Here's how: $y$.  No extra ink is needed.\n",
    "\n",
    "Ok, so now we know the following:  \n",
    " * **$y$**: actual y  \n",
    " * **$\\overline{y}$**: estimated y\n",
    " \n",
    "Now, using the Greek letter $\\varepsilon$, epsilon, to indicate error, we can say $\\varepsilon = y - \\overline{y}$.  And, we can be a little more precise by saying we are talking about error at any specific point, where $y$ and $\\overline{y}$ are at that $x$ value.  This is written as: \n",
    "\n",
    "$\\varepsilon _{i}$ = $y_{i}$ - $\\overline{y}_{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, applying this to a specific point of say when $ x = 100 $, we can say  $\\varepsilon _{x=100} = y_{x=100}$ - $\\overline{y}_{x=100} = 150 - 250 = 150$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating and representing total error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now so far, we know how to calculate the error at a given value of $x$, $x_i$, by using the formula, $\\varepsilon_i$ = $y_i - \\overline{y_i}$.  And this helpful at describing how well our regression line predicts the value of $y$ at a specific point.  \n",
    "\n",
    "However, we want to see well our regression describes our dataset in general - not just at a given point.  So let's move beyond calculating the error at a given point to describing the total error of the regression line across all of our data.  As an initial approach, we simply calculate the total error by summing the errors, $y - \\overline{y}$, for every point in our dataset.  \n",
    "\n",
    "Total Error = $\\sum_{i=1}^{n} y_i - \\overline{y_i}$\n",
    "\n",
    "This isn't bad, but we'll need to modify this approach slightly. To understand why, let's take another look at our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./regression-scatter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Take a look at what happens if we add the errors at $x = 100$ and $x = 200$. \n",
    "\n",
    "* $\\varepsilon_{x=100}= 150 - 250 = -100$\n",
    "* $\\varepsilon_{x=200} = 600 - 400 = 200$  \n",
    "* $\\varepsilon_{x=100} + \\varepsilon_{x=200} =  -150 + 200 = 50 $\n",
    "\n",
    "So because $\\varepsilon_{x=100}$ is positive while $ \\varepsilon_{x=200} $ is negative, adding the two errors  begins to cancel them out.  That's not what we want.  To prevent this from happening, we can simply square the errors, which ensures that we are always summing positive numbers.\n",
    "\n",
    "${\\varepsilon_i^2}$ = $({y_i - \\overline{y_i}})^2$\n",
    "\n",
    "Now given a list of points with coordinates (x, y), we can calculate the squared error of each of the points, and sum them up.  This is called our ** residual sum of squares ** (RSS).  Using our sigma notation, our formula RSS looks like: \n",
    "\n",
    "RSS $ = \\sum_{i = 1}^n ({y_i - \\overline{y_i}})^2$\n",
    "\n",
    "\n",
    "\n",
    "So let's apply this to our example.  In our example, we have actual $x$ and $y$ values at the following points: $ (0, 100), (100, 150), (200, 600), (400, 700) $.  And we can calculate the values of $\\overline{y} $ as $\\overline{y} = 1.5 *x + 100 $, for each of those four points.  So this gives us:\n",
    "\n",
    "RSS = $(0 - 0)^2 + (150 - 250)^2 + (600 - 400)^2 + (700 - 700)^2$ \n",
    "\n",
    "which reduces to  \n",
    "\n",
    "$-100^2 + 200^2 = 50,000$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this is great.  So now we have one number, RSS, that does a good job of representing how well our regression line fits the data.  We got there by calculating the errors at each of our provided points, and then squaring the errors so that our errors are always positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous to this lesson, we have simply assumed that our regression lines make good predictions of $y$ for given values of $x$.  In this lesson, we aimed to find a metric to tell us how well our regression line fits our actual data.  To do this, we started looking at an error at a given point, and defined error as the actual value of $y$ minus the expected value of $y$ from our regression line.  Then we were able to describe how well our regression line describes the entire dataset by squaring the errors at each point (to eliminate negative errors), and adding these errors.  This is called the Residual Sum of Squares (RSS).  This is our metric for describing how well our regression line fits our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
